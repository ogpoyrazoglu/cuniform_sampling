<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="MoRF avatars are created from short monocular videos and can be rendered in real-time (30+ FPS, 640x640px) on mobile devices">
    <meta property="og:title" content="C-Uniform Trajectory Sampling For Fast Motion Planning"/>
    <meta property="og:description"
          content="C-Uniform Trajectory Sampling For Fast Motion Planning"/>
    <meta property="og:url" content="https://github.com/ogpoyrazoglu/cuniform_sampling"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="C-Uniform Trajectory Sampling For Fast Motion Planning">
    <meta name="twitter:description"
          content="MoRF avatars are created from short monocular videos and can be rendered in real-time (30+ FPS, 640x640px) on mobile devices">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>C-Uniform Trajectory Sampling For Fast Motion Planning</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        <a style="color:black;">C-Uniform Trajectory Sampling For Fast Motion Planning</a>
                    </h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/goktug.jpg" alt="">
                            </div>
                            <a href="https://www.linkedin.com/in/ogpoyrazoglu/" target="_blank">O. Goktug<br>Poyrazoglu</a>
                        </span>

                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/yukang.jpg" alt="">
                            </div>
                            <a href="https://www.linkedin.com/in/yukang-cao-012177220/" target="_blank">Yukang<br>Cao</a>
                        </span>

                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/volkan.jpg" alt="">
                            </div>
                            <a href="https://scholar.google.com/citations?user=Q5KT-hEAAAAJ&hl=en" target="_blank">Volkan<br>Isler</a>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">Robotic Sensor Network Lab, University of Minnesota</span>
                    </div>

                    <!-- ArXiv Link -->
                    <span class="link-block">
                    <a href="" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                    </a>
                    </span>

                    <!-- Github link -->
                    <br>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="container">
                We study the problem of sampling robot trajectories and introduce the notion of C-Uniformity. As opposed to the standard method of uniformly sampling control inputs
(which lead to biased samples of the configuration space), C-Uniform trajectories are generated by control actions which
lead to uniform sampling of the configuration space. After presenting an intuitive closed-form solution to generate C-
Uniform trajectories for the 1D random-walker, we present a network-flow based optimization method to precompute C-
Uniform trajectories for general robot systems. We apply the notion of C-Uniformity to the design of Model Predictive Path
Integral controllers. Through simulation experiments, we show that using C-Uniform trajectories significantly improves the
performance of MPPI-style controllers, achieving up to 40% coverage performance gain compared to the best baseline. We
demonstrate the practical applicability of our method with an implementation on a 1/10th scale racer.
                <br>
                <br>
            </div>
            <div class="container" style="text-align:center;">
                <img src="static/images/teaser.png" alt="Main Idea" style="text-align:center;width:659px;"/>
            </div>
        </div>
    </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">

            <h2 class="title has-text-centered">Supplementary Video</h2>

            <div style="text-align: center;">
                <video poster="" id="demo1" autoplay controls muted loop playsinline width="75%" height="75%">
                    <source src="./static/videos/IROS24_2408_VI_i.mp4" type="video/mp4">
                </video>
            </div>
            <!-- End image carousel -->
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- <section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div style="text-align: center;">
                <h2 class="title has-text-centered">Planning results</h2>
                <img src="./static/videos/anim1.gif" alt="Path Planning " width="50%">
                <img src="./static/videos/anim2.gif" alt="Path Planning " width="50%" style="float:left">
            </div>
            <div style="text-align: center;">
                Planning based human motion prediction. Greedy v.s. Dynamic programming.
            </div>
        </div>
    </div>
</section> -->

<!--<div class="topnav">-->
<!--    <a href="#home">Home</a>-->
<!--    <a href="#news">Download</a>-->
<!--    <a href="#contact"><a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">Github</a>-->
<!--        <a href="#about"><a href="https://rsn.umn.edu/">About</a>-->
<!--</div>-->

<!--<div class="container">-->
<!--    <h1>Map-Aware Human Pose Prediction for Robot Follow-Ahead</h1>-->
<!--    <p><strong>Authors:</strong> Qingyuan Jiang, Burak Susam, Jun-Jee Chao, and Volkan Isler</p>-->
<!--    <p><strong>University of Minnesota RSN Laboratory</strong></p>-->

<!--    <h2>Introduction</h2>-->
<!--    <p>-->
<!--        We address the challenge of maintaining a robot's position ahead of-->
<!--        a moving actor by predicting the actor's 3D pose and movements in complex environments.-->
<!--        We propose a novel method that integrates environmental information for accurate,-->
<!--        long-term human pose prediction, overcoming limitations of existing methods in open spaces.-->
<!--        They introduce a new, realistic dataset for human motion in large areas and a robot system-->
<!--        equipped with dual cameras for localization and motion tracking. Our approach outperforms-->
<!--        existing models in trajectory and pose prediction, demonstrating real-time application in-->
<!--        robot follow-ahead tasks without relying on external cameras. Key contributions include a-->
<!--        real-time prediction method using occupancy maps and GRU, a new large-scale dataset,-->
<!--        and superior performance in both trajectory prediction and long-term human pose forecasting.-->
<!--    </p>-->
<!--    <figure>-->
<!--        <img src='static/images/teaser.png' class="responsive-image">-->
<!--        <figcaption>Figure 1: Map-aware Human Pose Prediction</figcaption>-->
<!--    </figure>-->

<!--    <h2>Approach</h2>-->
<!--    <p>Given human history poses and surrounding environment,-->
<!--        our method predicts the future poses by following a twostep pipeline: first predict a human-->
<!--        trajectory, then complete the full-body pose based on the-->
<!--        predicted trajectory. The following subsections introduce our-->
<!--        representation of the environment and human poses. Then,-->
<!--        we describe each component in our network architecture in-->
<!--        detail.</p>-->
<!--    <figure>-->
<!--        <img src='static/images/supplemantary/modules.png' class="responsive-image">-->
<!--        <figcaption>Figure 2: System Modules</figcaption>-->
<!--    </figure>-->
<!--    <figure>-->
<!--        <img src='static/images/network.png' class="responsive-image">-->
<!--        <figcaption>Figure 3: Network Architecture</figcaption>-->
<!--    </figure>-->

<!--    <h2>Dataset</h2>-->
<!--    <p>In addition to the standard synthetic dataset: GTA-IM,-->
<!--        we are also interested in evaluating our method on a largescale real-world dataset. However, existing realistic-->
<!--        datasets,-->
<!--        such as PROX [9], are limited to single-room areas. Therefore, we collect and propose the Real Indoor Motion-->
<!--        (RealIM) dataset for the large-scale human follow-ahead. We-->
<!--        simultaneously capture the entire human body and environment in building-scale spaces using the robot shown in-->
<!--        Figure 4. Compared to the existing synthetic dataset (GTAIM), the proposed dataset contains more movement-->
<!--        patterns,-->
<!--        including walking, crab moving, and varying moving speeds.-->
<!--        We collect 12 sequences from 5 different building halls.-->
<!--        Each contains an approximately four-minute movement. The-->
<!--        dataset includes sequences with different lighting conditions-->
<!--        recorded at different daytime. We invite multiple actors with-->
<!--        different walking styles and genders. We provide the raw-->
<!--        ROS bags and pre-processed data for direct use. In Fig. ??,-->
<!--        we present a few sample images from our dataset visualized-->
<!--        in Rviz</p>-->
<!--    <figure>-->
<!--        <img src='static/images/hardware.png' class="responsive-image">-->
<!--        <figcaption>Figure 4: Hardware Setup</figcaption>-->
<!--    </figure>-->
<!--    <figure>-->
<!--        <img src='static/images/dataset.png' class="responsive-image">-->
<!--        <figcaption>Figure 5: The Dataset and Collection Stages</figcaption>-->
<!--    </figure>-->


<!--    <h2>Results</h2>-->

<!--    <figure>-->
<!--        <img src='static/images/qualitative.png' class="responsive-image">-->
<!--        <figcaption>Figure 6: Qualitative results - Placeholder</figcaption>-->
<!--    </figure>-->

<!--    <section>-->
<!--        <h2>Setup</h2>-->
<!--        <p>After cloning the <a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">repo</a>, download the-->
<!--            data and place it in the data/ folder.</p>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Preprocess</h2>-->
<!--        <p>Execute <code>python process_real_dataset.py</code> to preprocess the data. This creates a <code>pf_preprocessed</code>-->
<!--            folder inside the data folder.</p>-->
<!--        <pre>-->
<!--        humanPosePrediction-->
<!--        ├── data-->
<!--        │   ├── Real-IM-Dataset-->
<!--        │   │   ├── sqeuence-1-->
<!--        │   │   │   ├── img1.jpg-->
<!--        │   ├── pf_preprocessed-->
<!--        │   │   ├── sequence-1.npz-->
<!--        ├── datasets-->
<!--        ├── results-->
<!--        </pre>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Training</h2>-->
<!--        <p>Initiate training by running <code>python train.py</code>. To train with more networks, add scripts in the-->
<!--            models folder and update the <code>models/motion_pred.py</code> file accordingly.</p>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Evaluation</h2>-->
<!--        <p>For evaluation, run <code>python eval.py</code> to visualize the results.</p>-->
<!--    </section>-->

<!--    <h2>References</h2>-->
<!--    <ul>-->
<!--        <li>A list of references used throughout the paper.</li>-->
<!--    </ul>-->
<!--</div>-->
</body>
</html>